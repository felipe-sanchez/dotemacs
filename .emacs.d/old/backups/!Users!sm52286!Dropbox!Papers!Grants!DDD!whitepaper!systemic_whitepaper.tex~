\documentclass[justified]{tufte-handout}

\usepackage{upgreek}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{titlesec}

\titleformat{\section}
{\color{darkgray}\normalfont\Large\bfseries}
{\color{red}\thesection}{1em}{}

\definecolor{light-gray}{gray}{0.95}
\lstset{
	 backgroundcolor=\color{light-gray},
	  breaklines=true,
showstringspaces=false,
frame=single,
numbers=left
}
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\aj}{Astronomical Journal}
\newcommand{\apj}{Astrophysical Journal}
\newcommand{\mnras}{MNRAS}
\newcommand{\pasp}{PASP}
\newcommand{\cc}[1]{\texttt{#1}}
\newcommand{\smthanks}[1]{{\small $^{#1}$}}
\newcommand{\Systemic}{\textcolor{Cerulean}{\href{http://www.stefanom.org/systemic}{Systemic2}}}
\newcommand{\ucolick}{UCO/Lick Observatory, University of California at Santa Cruz}
\newcommand{\carnegie}{Carnegie Institute of Washington}
\newcommand{\mcdonald}{McDonald Observatory, University of Texas at Austin}
\newcommand{\hyper}[2]{\textcolor{Cerulean}{\href{#1}{#2}}}

\title{Searching for New Worlds with Systemic2}
\author{Stefano Meschiari, Gregory P. Laughlin, Steve S. Vogt\smthanks{2}, Paul R. Butler, Jennifer Burt\smthanks{2}, Russell Hanson\smthanks{2}, Joel Green\smthanks{1}}
\begin{document}
\maketitle

\begin{abstract}
We present Systemic Console 2, a new software package specifically written for the analysis of exoplanetary time series (radial velocity and transit timing). We review some of its data analysis workflows, present applications to real exoplanetary data, and finally discuss a few planned features for the software.\footnote{This document is a ``live'' document, and will be updated as the software evolves.}
\end{abstract}

\section{Motivation}
Development of astronomical software has become a crucial part of professional astronomers' work in recent years (ref.). This trend has been in part a reaction to the ever-increasing stream of data being produced from disparate observatories in multiple wavelengths (ref.), and in part because of the need for more sophisticated tools for data analysis (ref.), astrophysical simulations (ref.), and modelling (ref.). The community at large is also recognizing the importance of open-sourcing their software. As astronomical codes get increasingly complex, open-source projects have several advantages, such as transparency (ref.), flexibility and increased potential for adaptation. Adopting an open-source approach to write software benefits the entire community, avoiding the need to ``reinvent the wheel'' to solve common needs and exponentially increasing the scientific return on the team's time investment. During a recent meeting of the American Astronomical Society\footnote{\hyper{http://aas.org/posts/story/2014/01/astrophysics-code-sharing-ii-sequel}{Astrophysics Code Sharing, the sequel}}, it was argued that ``\textit{widely used software has enabled at least as much science as a new instrument would}''. However, it was also noted that ``\textit{nothing within the funding agencies offer support for software development}'', highlighting a fundamental issue in astronomical software development. We believe this mismatch between funding requirements and modern astrophysical practice will be self-correcting, as sophisticated astronomical software will be more and more critical for modern astrophysics.

In keeping with the notions above, our group recently released a new version of the \Systemic{} Console. It is a completely rewritten version of the widely used Systemic package \citep[the previous version is now called Systemic 1, in order to distinguish it from the rewritten package;][]{Meschiari09, Meschiari10}. The \Systemic{} package enables the user to quickly and interactively prototype the analysis and fitting of exoplanetary time series, including radial velocity (RV) and timing data. It consists of a fast, parallelized C library (which can be easily integrate into existing projects), an R package providing a high-level scripting interface to the library and easy integration with other \hyper{http://cran.us.r-project.org}{CRAN} packages, and an easy-to-use  ``studio'' user interface for interactive use and plotting. A Python interface is also in the works.


\section{Overview}
\begin{marginfigure}
\includegraphics[width=\linewidth]{scheme.pdf}
\caption{A schematic view of the architecture of Systemic, in order of increasing ``complexity'' for the user (from top to bottom). There are three ways to use Systemic: (1) Using the graphical user interface (GUI); (2) Using the high-level R interface (Python and IDL interfaces are in the works); (3) Linking to the C library. Each level is dependents on the next; at the lowest level, the Systemic library makes use of several battle-tested, open source packages.}\label{fig:schematic}
\end{marginfigure}

In this section, we give a quick overview of the data-analysis tools offered by the \Systemic{} package. Each of these tools may be accessed in three different ways, depending on the user's expertise: (1) through the ``studio'' graphical user interface (GUI); (2) through the R package; and (3) linking to the C library. Figure \ref{fig:schematic} shows a cartoon of the hierarchy of the \Systemic{} code; the user can choose their preferred entry proint (GUI, R, or C).

Some of the features offered by the \Systemic{} package are:
\begin{itemize}
\item Interactive fitting of radial velocity and transit timing data. The fitting model include three-dimensional planetary orbital parameters, offsets, linear and quadratic trends, and user-specified terms.
\item Lomb-Scargle periodograms with bootstrapped false alarm probabilities (FAP).
\item Self-consistent dynamical fitting using a builtin integrator (Runge-Kutta 8/9 or Bulirsch-Stoer) or a custom integrator supplied by the user.
\item Optimization using one of the Simplex, Levenberg-Marquardt, Simulated Annealing or Differential Evolution algorithms.
\item Error estimation using Bootstrap, Markov-Chain Monte Carlo (MCMC), or custom routines.
\item Simple facilities for input/output, manipulation and cleaning of data.
\item Long-term dynamical integration with SWIFT's RMVS algorithm.
\item Cross-validation (jack-knife) for model comparison. Model comparison can also be accomplished with one of the R packages for MCMC.
\end{itemize}
In the following sections, we will highlight some of the sophisticated features that are easily accessible within \Systemic{}.

\subsection{User interface}
The user interface strives to be an easy to use, interactive IDE (Integrated Development Environment) for the analysis of exoplanetary data. Sophisticated workflows can be initiated by point-and-click and menu items, and are readily configurable. Plots update automatically in response to parameter changes (e.g. the orbital elements of a planet), providing the user with an up-to-date view of the data and the model fit. The user can model multiple data sets simultaneously using a tabbed interface, or multiple windows.



\subsection{R scripting}

\section{Markov Chain Monte Carlo error estimation}
\subsection{Algorithm}
Systemic2 includes a simple implementation of the MCMC algorithm \citep{Ford05, Ford06, Balan09, Meschiari09, Gregory11}. The algorithm returns an chain of state vectors $\ve{k_i}$ (a set of coupled orbital elements -- e.g. period, mass, ... -- and parameters -- e.g. velocity offsets, RV trend, ... -- which are called ``kernels'' in the Systemic code parlance).\footnote{In this section, I will use ``parameter'' to refer to both orbital elements (e.g. period, mass, ...) and non-planetary parameters (velocity offsets, trends, jitter, ...). Lower subscripts will refer to the index of a state vector within the chain, upper subscripts to the index within } The target distribution of the chain is an equilibrium distribution proportional to $\exp\left[-\chi^2(\ve{k})\right]$.

The single-threaded algorithm can be summarized as follows:
\begin{enumerate}
\item Initialize the routine with an input state vector $\ve{k_0}$ (normally, a kernel object with associated data already loaded) and a scale vector $\ve{B}$ (see below). Only parameters that are flagged with the MINIMIZE flag (equivalently, selected in the GUI) are varied, while the remaining are held constant.
\item Given a state $\ve{k_i}$, generate a trial $\ve{k_{i+1}}$ using a transition function $q(\ve{k_{i+1}}|\ve{k_i})$. $q$ varies each element $j$ of $\ve{k_i}$ using a random Gaussian number generator centered on $\ve{k_i^j}$ with scale $\ve{B^j}$.\footnote{The algorithm respects any ranges set on parameters; e.g. if in the GUI or code a maximum and minimum mass is set on the input state vector, the MCMC algorithm will reject them so the distribution is cropped accordingly.}
\item The new trial state $\ve{k_{i+1}}$ is accepted according to a transition probability $r$ given by
\begin{equation}
r = \min\left(1, \frac{p(\ve{k_{i+1}}|D, I)}{p(\ve{k_{i}}|D, I)}\right)
\end{equation}
where $p(\ve{k}|D, I)$ is the posterior probability that model $\ve{k}$ is true given the data D (e.g. the RV measurements) and prior information I. This probability is also called ``merit function'' in the code -- a better model has a higher value of the merit function.

Bayes' theorem states that
\begin{equation}
p(\ve{k}|D, I) \propto p(\ve{k}|I) L(D|\ve{k}, I)
\end{equation}
$p(\ve{k}|I)$ is the assumed prior distribution, which is the product of the prior distribution $p^j$ for each parameter $j$.\footnote{e.g. a uniform random distribution between 0 and $2\pi$ for the mean anomaly} $L(D|\ve{k}, I)$ is the likelihood function (i.e. the probability of getting the data D given the parameter set $\ve{k}$). For Gaussian errors, this is given by
\begin{equation}
L(D|\ve{k}, I) = A \exp\left(-\sum_n^N \frac{d_n - f_n(\ve{k})}{2(\sigma_n^2+s^2)}\right)
\end{equation}
\begin{equation}
A = (2\pi)^{-N/2} \left(\prod_n^N \sqrt{\sigma_n^2+s^2}\right)
\end{equation}
where $d_n$ is the n-th data point and $f_n$ is the n-th prediction of the model. $\sigma_n$ is the reported uncertainty for the n-th data point and $s$ represents error not accounted for by the reported uncertainty.\footnote{s corresponds to ``jitter''/under-reported noise added in quadrature and is also a varying parameter in the model. In the implementation, each dataset has a separate s parameter, reflecting differences in telescopes or type of data.}
\item Sample a random uniform number $u$; if $u < r$, accept the new state $\ve{k_{i+1}}$ and append it to the chain, otherwise reject it and take $\ve{k_{i+1}} = \ve{k_i}$.
\item Stop if a convergence criterion is achieved; otherwise, loop to step 2.
\end{enumerate}

The algorithm presented above leaves out the following details: (1) choice of the input state $\ve{k_0}$; (2) the scale vector $\ve{B}$; (3) prior probabilities for each parameter of the model; (4) stopping criterion for the algorithm.

\begin{description}
\item[Input state.] The input state represents the starting point for the algorithm, thus setting the starting value of each parameter. If the algorithm is started very far from the most interesting regions of the parameter space, a large fraction of computational time will be spent exploring very high $\chi^2$ regions, possibly never converging to interesting minima (best fits). Conversely, starting from values corresponding to best fits can result in the algorithm not exploring alternative solutions because minima are too deep to escape. The implementation in Systemic2 allows the user to run multiple chains in parallel, with different starting states, and subsequently collates the chains at the end of the run; with enough chains, the parameter space should be exhaustively explored, at the cost of computational time. The user has to provide the starting states.\footnote{The optimal starting states will depend on the data. If, for instance, the data supports three periodicities, the user could use the three periods as starting points. A genetic algorithm could also be used to generate initial states.}
\item[Scale vector.] The scale vector sets the acceptance ratio, e.g. the fraction of trial states that are accepted vs. rejected (see step 3). Large $B^j$ (``big steps'' in the parameter space) will likely reduce the acceptance ratio and increase churning (steps are generated and evaluated but discarded), but improve the chance of escaping minima; vice-versa for small $B^j$ (``small steps''). The optimal target acceptance ratio is 0.44.

The default implementation in Systemic2 starts with default values of $B$ for each parameter. Before starting the main iteration, it runs the above algorithm for a limited number of steps, varying each $B^j$ until acceptance ratio within 1\% of the target acceptance ratio is achieved. Note that for sufficiently pathological systems, this procedure might not converge and the user will have to supply the value of $B^j$ by hand. In the case of $\chi^2$ landscapes with many minima, if the algorithm later converges to a deep minimum after spending the initial portion of the run in a high-$\chi^2$ region, the values of $\ve{B}$ might not be suitable anymore for the new position in the parameter space. The solution to this issue would be a combination of parallel tempering (partially implemented) and automated scaling of $B^j$ (not implemented); for now, the user has to monitor the output of the routine and adjust accordingly.

\item[Priors.] Systemic2 (by default) uses non-informative priors for all parameters. Priors should not strongly influence the result, unless the data is poor. Non-informative priors are, respectively:
\begin{itemize}
\item Uniform prior: for all angles (between 0 and $2\pi$), eccentricity,  and other parameters (including velocity offsets). Minimum and maximum values are set by setting the range on the input kernel object. $p(x) \propto 1/(x_{max}-x_{min})$
\item Uniform prior in log (Jeffreys): for mass and period. $p(x) \propto 1/[(x+x_{min}) \log(x_{max}/x_{min})]$
\end{itemize}

\item[Stopping criteria.] Systemic2 provides an automated stopping criterion based on the Gelman-Rubin statistic $\hat{R}$, as described in \citep{Ford06}, which converges to 1 as the chain converges. This statistic attempts to evaluate whether the MCMC run has converged and further iterations are not necessary (however, convergence cannot be assured); it is a measure of the dispersion in each parameter within a chain and across different chains. The run is periodically interrupted to calculate $\hat{R}$; by default, the algorithm is considered converged when $\hat{R} < 1.1$ or the value entered by the user.
\end{description}

\subsection{Default C implementation}
The default C implementation is contained in the file \cc{mcmc.c} (with header file \cc{mcmc.h}). The source file contains a number of functions, mainly \cc{K\_mcmc\_single} which implements the algorithm above, and \cc{K\_mcmc\_mult} which wraps \cc{K\_mcmc\_single} and implements multiple parallel chains (using multiple cores) and the stopping criterion. It is recommended that only \cc{K\_mcmc\_mult} is used.

This is the definition of the function and its parameters:\\
\begin{lstlisting}[language=C]
ok_list* K_mcmc_mult(ok_kernel** k, unsigned int nchains, unsigned int ntemps, unsigned int skip, unsigned int discard, const double params[], double Rstop, ok_callback2 merit_function);
\end{lstlisting}

\begin{itemize}
\item \cc{k} is an array of \cc{ok\_kernel} objects. There should be at least \cc{nchains} elements in the array. Each element of \cc{k} is the starting state for each chain.
\item \cc{nchains} sets the number of chains that will be run concurrently. It is a good idea to start with a small number (e.g. 2) and evaluate convergence with additional runs with more chains.

Multiple threads are automatically spawned, so each chain runs on its own thread (so that on an 8-core machine, up to 8 chains can run at 100\% speed). Communication between threads is handled transparently.
\item \cc{ntemps} is used for parallel tempering and would specify the number of temperature levels \citep{Gregory05_book}. This is implemented but not very well tested; set to 1 for now.
\item \cc{skip} sets how many items to discard from the head of the chain. It is customary to remove the first N items (e.g. 1,000) to avoid correlation with the starting state.
\item \cc{discard} sets how often to save states for output. The routine will only retain in memory every \cc{discard} (e.g. 100th) element; this minimizes correlations between subsequent states. A rule of thumb is \cc{discard} = 10 * number of parameters that are being varied. 
\item \cc{params} is a double array representing additional, less important parameters; pass NULL for default values. The array should be set as a sequence of pairs of  ``sentinel'' constants (called \cc{OPT\_MCMC\_*} and defined in \cc{systemic.h}) and values. The last element of the array must be the \cc{DONE} constant. Here are some of the parameters implemented:
\begin{itemize}
\item \cc{OPT\_MCMC\_NSTOP}: maximum number of iterations before stopping (default: no maximum, iteration is stopped when convergence is achieved according to $\hat{R}$)
\item \cc{OPT\_MCMC\_NMIN}: absolute minimum length of the output chain; even if the algorithm is convergence, produce at least these many samples (default: 5000)
\item \cc{OPT\_MCMC\_ACCRATIO}: choose a different acceptance ratio (default: 0.44)
\item \cc{OPT\_MCMC\_SKIP\_STEPS}: if set to 1, skips the automatic scaling of $B^j$
\item \cc{OPT\_MCMC\_VERBOSE\_DIAGS}: outputs some diagnostic information (if set to 2) or lots of diagnostic information (if set to 3)
\end{itemize}
\item \cc{Rstop}: stops the algorithm when $\hat{R} <$\cc{Rstop}. Set to a number close to 1 (e.g. 1.1).
\item \cc{merit\_function}: set to NULL for the default set of prior and likelihood calculation (as defined in the previous section), or specify a pointer to a function for a custom merit function. The default merit function is defined in \cc{mcmc.c} as \cc{K\_mcmc\_merit\_default}; look at that function as a starting point.
\end{itemize}

The routine returns a pointer to an object (struct) of type \cc{ok\_list}; each element of the list is an element of the output chain. Marginal distributions and other summary statistics can then be built from the chain.

In the following sections I will show how to use the MCMC routine, from the highest level (the GUI or R) to the lowest level (C).

\subsection{Example - Calling from the GUI/R}
\begin{itemize}
\item Launch the GUI and load a new  dataset, for instance the 61 Vir dataset (HD115617). Find the best-fit, 3-planet system.\footnote{
You can set up ranges for each parameter by right-clicking on the parameter labels (e.g. the ``Period [d]'' label).
}
\item Click on the bulls-eye icon, select the kernel object to analyze, choose MCMC under Type and click Setup. The command call will be setup for you with sensible values.\\
\item Press RETURN. Systemic will start by computing the appropriate step sizes first. This step requires approximately 4 minutes on my MacBook Pro (2012), using two chains. 
\item After computing the steps, the actual iteration will start. The total time to achieve convergence will vary (approximately 15 minutes).
\item Once the routine returns, summary statistics will be printed. The chain has been saved in \cc{k\$errors} (if the kernel name was k). Select it in the Error Estimation panel.\\
\item From the panel, you can plot: histograms, scatter plots, contours and smooth scatters. You can also access the chain directly and make your own plots.\\
\includegraphics[width=5cm]{3.pdf}\includegraphics[width=5cm]{4.pdf}
\end{itemize}

\subsection{Example - Calling from C}
Below is a sample minimal code calling the MCMC routine, with a step-by-step explanation below. This code should be linked against the systemic library. It is included in src/test.c.

\begin{lstlisting}[language=C]
#include <stdio.h>
#include "systemic.h"
#include "kernel.h"
#include "mcmc.h"

int progress(int current, int max, void* state, const char* msg);

int main() {
    // Open the best fit for 61 Vir (1)
    FILE* fid = fopen("hd115617.fit", "r");
    ok_kernel* k = K_load(fid, 0);
    
    // Set the callback function for k to function "progress" (2)
    K_setProgress(k, progress);
    
    // Activate the noise parameter (3)
    K_setParFlag(k, P_DATA_NOISE1, ACTIVE | MINIMIZE);
    K_setParFlag(k, P_DATA_NOISE2, ACTIVE | MINIMIZE);
    
    // Create two different starting states from perturbations
    // of k (4)
    K_perturb(k);
    
    ok_kernel* k2 = K_clone(k);
    K_perturb(k2);
    
    ok_kernel* kk[2] = {k, k2};
    
    // Call the MCMC routine, return the result in kl
    ok_list* kl = K_mcmc_mult(kk, 2, 1, 1000, 200, (double[3]) {OPT_MCMC_NMIN, 5000, DONE},
            1.1, NULL);
    
    // Calculate the median and mean absolute deviation
    // of the orbital elements... (5)
    printf("Final chain length: %d\n", kl->size);
    
    gsl_matrix* med = KL_getElementsStats(kl, STAT_MEDIAN);
    gsl_matrix* mad = KL_getElementsStats(kl, STAT_MAD);
    char* labels[5] = {"Period", "Mass", "MA", "Ecc", "Long. peri"};
    
    // ... and print it out.
    for (int i = 1; i < K_getNplanets(k)+1; i++) {
        for (int j = PER; j <= LOP; j++)
                printf("%s [%d]: %e +- %e\n", labels[j], i,
                        MGET(med, i, j), MGET(mad, i, j));                   
    }
    
    // Finally, save it to file.
    FILE* fid2 = fopen("hd115617.kl", "w");
    KL_save(kl, fid2);
    fclose(fid2);
    
}

int progress(int current, int max, void* state, const char* msg) {
    if (! strcmp(msg, "") == 0)
        printf("%s [%d/%d]\n", msg, current, max);
    return PROGRESS_CONTINUE;
}

\end{lstlisting}
\begin{enumerate}
\item Creates a new kernel \cc{k} from a fit file created with the GUI named "hd115617.fit". In Systemic2, fit files contain both the orbital elements/parameters and the data associated with it.
\item Kernel objects can have ``callbacks'' associated with it. Callbacks are used by long-running routines to give the user the option of display progress or stopping the routine altogether. In this listing, a function named \cc{progress} displays diagnostic messages from the MCMC routine.
\item We allow the noise parameter $s$ to float. This parameter is called \cc{P\_DATA\_NOISE} in the code, and there are separate ones for each data set (up to 10); since there are two data sets (KECK and APF), we allow both to float. (The GUI/R side takes care of this automatically). Independent $s$ parameters can help pinpoint troublesome interactions between different datasets \citep[e.g.][]{Gregory11}.
\item We create two instances of the same kernel with slightly perturbed orbital elements and parameters. The \cc{K\_perturb} function perturbs a kernel.\footnote{The amount of the perturbation is set using the \cc{K\_setElementStep} and \cc{K\_setParameterStep} functions. There are default values for each parameter in a kernel object.}
\item Call the MCMC routine using the prototype shown in the Implementation section. The starting states are the two perturbed kernels (each corresponding to one chain), we skip the first 1,000 states, only keep one out of 200 states (discard the other 199) to minimize correlations, ask for a chain with at least 5,000 states, and finally ask for Gelman-Rubin convergence better than 1.1.
\item Here I show a simple summary statistic calculated with the output chain. I calculate the median and mean absolute deviation of the orbital elements of the states within the output chain, then print it out.\footnote{Functions in kl.c can be used for simple summary statistic calculations; otherwise, R/Python have a great selection of statistical routines to calculate interesting quantities. Chains saved with \cc{KL\_save} are in a simple tabular format, so they are easy to import.}
\item Finally, I save the output for later consumption.
\end{enumerate}


\bibliographystyle{apj}
\bibliography{../../../Articles/biblio}
\end{document}